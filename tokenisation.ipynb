{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LLM-Powered Apps from Weights & Biases\n",
    " <br>Here, they are using Open AI but we'll we use free or <br> almost free models from Open Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll be using Llama 3.18B Instuct(Free) and \n",
    "# Gemini 2.0 Flash \n",
    "## Both were launched in 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastapi 0.95.1 requires pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2, but you have pydantic 2.10.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai tiktoken wandb -qq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade openai==0.27.2 tiktoken wandb -qq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "load_dotenv()  # Loads variables from .env into the environment\n",
    "openai.api_key = os.getenv(\"OPENROUTER_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv()  # Loads environment variables from a .env file\n",
    "\n",
    "# If you're using OpenRouter, set the API base accordingly:\n",
    "openai.api_base = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "# Set your API key from an environment variable\n",
    "openai.api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "# Now you can make API calls, for example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm an AI, and I'm not a specific model, but rather a general type of model known as a Conversational AI or a Dialog-based AI. I'm a product of the Meta AI family, which means I'm based on a versatile AI architecture that allows me to understand and respond to a wide range of questions and topics.\n",
      "\n",
      "However, I can tell you about some of the underlying models that power our Conversational AI technology. Our AI is built on top of a type of neural network called a transformer, which is particularly well-suited for processing and understanding human language.\n",
      "\n",
      "If you're interested in the specifics, I can give you some more technical details. Meta currently uses several models, including:\n",
      "\n",
      "1. **ERT (Encoder-RNN-T) model**: This is a traditional transformer-based model that is widely used in the industry. It's a self-supervised model that has been fine-tuned for a specific task.\n",
      "2. ** Conversation AI models (CAVA) model**: This is a more advanced transformer-based model that focuses on conversational AI specifically. It has been fine-tuned for tasks like dialogue generation and topic handling.\n",
      "3. **mT5 (Mainframe AI model)**: This is a more advanced model that leverages multi-task learning and focuses on varying amounts of input and output. It's highly versatile and has achieved state-of-the-art results in a wide range of applications.\n",
      "\n",
      "Please note that these names and specific models may change over time as our technology evolves. But rest assured that I'll always be here to help answer your questions to the best of my abilities!\n",
      "\n",
      "How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"meta-llama/llama-3.1-8b-instruct:free\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello, world, Which model are you!\"}\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/swastik/Job_Search_2025/Projects/Building_LLM_Apps/wandb/run-20250308_013801-y2ks95xe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rishav/llmaps/runs/y2ks95xe' target=\"_blank\">denim-vortex-1</a></strong> to <a href='https://wandb.ai/rishav/llmaps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rishav/llmaps' target=\"_blank\">https://wandb.ai/rishav/llmaps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rishav/llmaps/runs/y2ks95xe' target=\"_blank\">https://wandb.ai/rishav/llmaps/runs/y2ks95xe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wandb.integration.openai import autolog\n",
    "import wandb\n",
    "wandb.login()\n",
    "from wandb.integration.openai import autolog\n",
    "\n",
    "autolog({\"project\":\"llmaps\", \"job_type\": \"introduction_first_tutorial\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1135, 2337, 1222, 8436, 1386, 318, 7427, 0]\n",
      "Weights & Biases is awesome!\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "enc = encoding.encode(\"Weights & Biases is awesome!\")\n",
    "print(enc)\n",
    "print(encoding.decode(enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1135\tWe\n",
      "2337\tights\n",
      "1222\t &\n",
      "8436\t Bi\n",
      "1386\tases\n",
      "318\t is\n",
      "7427\t awesome\n",
      "0\t!\n"
     ]
    }
   ],
   "source": [
    "for token_id in enc:\n",
    "    print(f\"{token_id}\\t{encoding.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Shambhala",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
